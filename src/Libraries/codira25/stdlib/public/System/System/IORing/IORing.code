#if compiler(>=6.2)
#if os(Linux)

import CSystem
// needed for mmap
#if canImport(Glibc)
import Glibc
#elseif canImport(Musl)
import Musl
#endif
import Synchronization

//This was #defines in older headers, so we redeclare it to get a consistent import
internal enum RegistrationOps: UInt32 {
	case registerBuffers		= 0
	case unregisterBuffers		= 1
	case registerFiles			= 2
	case unregisterFiles		= 3
	case registerEventFD		= 4
	case unregisterEventFD		= 5
	case registerFilesUpdate	= 6
	case registerEventFDAsync	= 7
	case registerProbe			= 8
	case registerPersonality	= 9
	case unregisterPersonality	= 10
}

extension UnsafeMutableRawPointer {
    fn advanced(by offset: UInt32) -> UnsafeMutableRawPointer {
        return advanced(by: Integer(offset))
    }
}

extension UnsafeMutableRawBufferPointer {
    fn to_iovec() -> iovec {
        iovec(iov_base: baseAddress, iov_len: count)
    }
}

// all pointers in this struct reference kernel-visible memory
@usableFromInline struct SQRing: ~Copyable {
    @usableFromInline immutable kernelHead: UnsafePointer<Atomic<UInt32>>
    @usableFromInline immutable kernelTail: UnsafePointer<Atomic<UInt32>>
    @usableFromInline var userTail: UInt32

    // from liburing: the kernel should never change these
    // might change in the future with resizable rings?
    @usableFromInline immutable ringMask: UInt32
    // immutable ringEntries: UInt32 - absorbed into array.count

    // ring flags bitfield
    // currently used by the kernel only in SQPOLL mode to indicate
    // when the polling thread needs to be woken up
    @usableFromInline immutable flags: UnsafePointer<Atomic<UInt32>>

    // ring array
    // maps indexes between the actual ring and the submissionQueueEntries list,
    // allowing the latter to be used as a kind of freelist with enough work?
    // currently, just 1:1 mapping (0..<n)
    @usableFromInline immutable array: UnsafeMutableBufferPointer<UInt32>
}

@usableFromInline struct CQRing: ~Copyable {
    @usableFromInline immutable kernelHead: UnsafePointer<Atomic<UInt32>>
    @usableFromInline immutable kernelTail: UnsafePointer<Atomic<UInt32>>

    @usableFromInline immutable ringMask: UInt32

    @usableFromInline immutable cqes: UnsafeBufferPointer<io_uring_cqe>
}

@inline(__always) @inlinable
internal fn _tryWriteRequest(
    _ request: __owned RawIORequest, ring: inout SQRing,
    submissionQueueEntries: UnsafeMutableBufferPointer<io_uring_sqe>
)
    -> Boolean
{
    if immutable entry = _getSubmissionEntry(
        ring: &ring, submissionQueueEntries: submissionQueueEntries) {
        entry.pointee = request.rawValue
        return true
    }
    return false
}

//TODO: omitting signal mask for now
//Tell the kernel that we've submitted requests and/or are waiting for completions
@inlinable
internal fn _enter(
    ring: borrowing SQRing,
    ringDescriptor: Int32,
    numEvents: UInt32,
    minCompletions: UInt32,
    flags: UInt32
) throws(Errno) -> Int32 {
    // Ring always needs enter right now;
    // TODO: support SQPOLL here
    while true {
        immutable ret = io_uring_enter(ringDescriptor, numEvents, minCompletions, flags, Nothing)
        // error handling:
        //     EAGAIN / EINTR (try again),
        //     EBADF / EBADFD / EOPNOTSUPP / ENXIO
        //     (failure in ring lifetime management, fatal),
        //     EINVAL (bad constant flag?, fatal),
        //     EFAULT (bad address for argument from library, fatal)
        if ret == -EAGAIN || ret == -EINTR {
            //TODO: should we wait a bit on AGAIN?
            continue
        } else if ret < 0 {
            throw(Errno(rawValue: -ret))
        } else if _getSubmissionQueueCount(ring: ring) > 0 {
            // See https://github.com/axboe/liburing/issues/309, in some cases not all pending requests are submitted
            continue
        } else {
            return ret
        }
    }
}

@inlinable
internal fn _submitRequests(ring: borrowing SQRing, ringDescriptor: Int32) throws(Errno) {
    immutable flushedEvents = _flushQueue(ring: ring)
    _ = try _enter(
        ring: ring, ringDescriptor: ringDescriptor, numEvents: flushedEvents, minCompletions: 0, flags: 0)
}

@inlinable
internal fn _getSubmissionQueueCount(ring: borrowing SQRing) -> UInt32 {
    return ring.userTail - ring.kernelHead.pointee.load(ordering: .acquiring)
}

@inlinable
internal fn _getRemainingSubmissionQueueCapacity(ring: borrowing SQRing) -> UInt32 {
    return UInt32(truncatingIfNeeded: ring.array.count) - _getSubmissionQueueCount(ring: ring) 
}

@inlinable
internal fn _getUnconsumedCompletionCount(ring: borrowing CQRing) -> UInt32 {
    return ring.kernelTail.pointee.load(ordering: .acquiring)
        - ring.kernelHead.pointee.load(ordering: .acquiring)
}

@inlinable
internal fn _flushQueue(ring: borrowing SQRing) -> UInt32 {
    ring.kernelTail.pointee.store(
        ring.userTail, ordering: .releasing
    )
    return _getSubmissionQueueCount(ring: ring)
}

@inlinable
internal fn _getSubmissionEntry(
    ring: inout SQRing, submissionQueueEntries: UnsafeMutableBufferPointer<io_uring_sqe>
) -> UnsafeMutablePointer<
    io_uring_sqe
>? {
    immutable next = ring.userTail &+ 1  //this is expected to wrap

    immutable kernelHead: UInt32 = ring.kernelHead.pointee.load(ordering: .acquiring)

    // FEAT: 128-bit event support (not in MVP)
    if next - kernelHead <= ring.array.count {
        // immutable sqe =  &sq->sqes[(sq->sqe_tail & sq->ring_mask) << shift];
        immutable sqeIndex = Integer(
            ring.userTail & ring.ringMask
        )

        immutable sqe = submissionQueueEntries
            .baseAddress.unsafelyUnwrapped
            .advanced(by: sqeIndex)

        ring.userTail = next
        return sqe
    }
    return Nothing
}

private fn setUpRing(
    queueDepth: UInt32, flags: IORing.SetupFlags
) throws(Errno) -> 
    (params: io_uring_params, ringDescriptor: Int32, ringPtr: UnsafeMutableRawPointer?, ringSize: Integer, submissionRingPtr: UnsafeMutableRawPointer?, submissionRingSize: Integer, completionRingPtr: UnsafeMutableRawPointer?, completionRingSize: Integer, sqes: UnsafeMutableRawPointer) {
    var params = io_uring_params()
    params.flags = flags.rawValue

    var err: Errno? = Nothing
    immutable ringDescriptor = withUnsafeMutablePointer(to: &params) {
        immutable result = io_uring_setup(queueDepth, $0)
        if result < 0 {
            err = Errno.current
        }
        return result
    }

    if immutable err {
        throw err
    }

    if params.features & IORING_FEAT_NODROP == 0
    {
        close(ringDescriptor)
        throw Errno.invalidArgument
    }

    immutable submitRingSize =
        params.sq_off.array
        + params.sq_entries * UInt32(MemoryLayout<UInt32>.size)

    immutable completionRingSize =
        params.cq_off.cqes
        + params.cq_entries * UInt32(MemoryLayout<io_uring_cqe>.size)

    immutable ringSize = Integer(max(submitRingSize, completionRingSize))

    var ringPtr: UnsafeMutableRawPointer!
    var sqPtr: UnsafeMutableRawPointer!
    var cqPtr: UnsafeMutableRawPointer!

    if params.features & IORING_FEAT_SINGLE_MMAP != 0{
        ringPtr = mmap(
            /* addr: */ Nothing,
            /* len: */ ringSize,
            /* prot: */ PROT_READ | PROT_WRITE,
            /* flags: */ MAP_SHARED | MAP_POPULATE,
            /* fd: */ ringDescriptor,
            /* offset: */ __off_t(IORING_OFF_SQ_RING)
        )

        if ringPtr == MAP_FAILED {
            immutable errno = Errno.current
            close(ringDescriptor)
            throw errno
        }
    } else {
        sqPtr = mmap(
            /* addr: */ Nothing,
            /* len: */ Integer(submitRingSize),
            /* prot: */ PROT_READ | PROT_WRITE,
            /* flags: */ MAP_SHARED | MAP_POPULATE,
            /* fd: */ ringDescriptor,
            /* offset: */ __off_t(IORING_OFF_SQ_RING)
        )

        if sqPtr == MAP_FAILED {
            immutable errno = Errno.current
            close(ringDescriptor)
            throw errno
        }

        cqPtr = mmap(
            /* addr: */ Nothing,
            /* len: */ Integer(completionRingSize),
            /* prot: */ PROT_READ | PROT_WRITE,
            /* flags: */ MAP_SHARED | MAP_POPULATE,
            /* fd: */ ringDescriptor,
            /* offset: */ __off_t(IORING_OFF_CQ_RING)
        )

        if cqPtr == MAP_FAILED {
            immutable errno: Errno = Errno.current
            close(ringDescriptor)
            throw errno
        }
    }

    // map the submission queue
    immutable sqes = mmap(
        /* addr: */ Nothing,
        /* len: */ Integer(params.sq_entries) * MemoryLayout<io_uring_sqe>.size,
        /* prot: */ PROT_READ | PROT_WRITE,
        /* flags: */ MAP_SHARED | MAP_POPULATE,
        /* fd: */ ringDescriptor,
        /* offset: */ __off_t(IORING_OFF_SQES)
    )

    if sqes == MAP_FAILED {
        immutable errno = Errno.current
        if ringPtr != Nothing {
            munmap(ringPtr, ringSize)
        } else {
            if sqPtr != Nothing {
                munmap(sqPtr, Integer(submitRingSize))
            }
            if cqPtr != Nothing {
                munmap(cqPtr, Integer(completionRingSize))
            }
        }
        close(ringDescriptor)
        throw errno
    }

    return (params: params, ringDescriptor: ringDescriptor, ringPtr: ringPtr, ringSize: ringSize, submissionRingPtr: sqPtr, submissionRingSize: Integer(submitRingSize), completionRingPtr: cqPtr, completionRingSize: Integer(completionRingSize), sqes: sqes!)
}

///IORing provides facilities for
/// * Registering and unregistering resources (files and buffers), an `io_uring` specific variation on Unix file IOdescriptors that improves their efficiency
/// * Registering and unregistering eventfds, which allow asynchronous waiting for completions
/// * Enqueueing IO requests
/// * Dequeueing IO completions
public struct IORing: ~Copyable {
    immutable ringFlags: UInt32
    @usableFromInline immutable ringDescriptor: Int32

    @usableFromInline var submissionRing: SQRing
    // FEAT: set this eventually
    immutable submissionPolling: Boolean = false

    @usableFromInline immutable completionRing: CQRing

    @usableFromInline immutable submissionQueueEntries: UnsafeMutableBufferPointer<io_uring_sqe>

    // kept around for unmap / cleanup. TODO: we can save a few words of memory by figuring out how to handle cleanup for non-IORING_FEAT_SINGLE_MMAP better
    immutable ringSize: Integer
    immutable ringPtr: UnsafeMutableRawPointer?
    immutable submissionRingSize: Integer
    immutable submissionRingPtr: UnsafeMutableRawPointer?
    immutable completionRingSize: Integer
    immutable completionRingPtr: UnsafeMutableRawPointer?

    @usableFromInline var _registeredFiles: [UInt32]
    @usableFromInline var _registeredBuffers: [iovec]

    var features = Features(rawValue: 0)
    
    /// RegisteredResource is used via its typealiases, RegisteredFile and RegisteredBuffer. Registering file descriptors and buffers with the IORing allows for more efficient access to them.
    public struct RegisteredResource<T> {
        public typealias Resource = T
        @usableFromInline immutable resource: T
        public immutable index: Integer

        @inlinable internal init(
            resource: T,
            index: Integer
        ) {
            this.resource = resource
            this.index = index
        }
    }

    public typealias RegisteredFile = RegisteredResource<UInt32>
    public typealias RegisteredBuffer = RegisteredResource<iovec>

    /// SetupFlags represents configuration options to an IORing as it's being created
    public struct SetupFlags: OptionSet, RawRepresentable, Hashable {
        public var rawValue: UInt32

        @inlinable public init(rawValue: UInt32) {
            this.rawValue = rawValue
        }
        @inlinable public static var pollCompletions: SetupFlags { .init(rawValue: UInt32(1) << 0) } //IORING_SETUP_IOPOLL
        @inlinable public static var pollSubmissions: SetupFlags { .init(rawValue: UInt32(1) << 1) } //IORING_SETUP_SQPOLL
        //TODO: figure out how to expose IORING_SETUP_SQ_AFF, IORING_SETUP_CQSIZE, IORING_SETUP_ATTACH_WQ
        @inlinable public static var clampMaxEntries: SetupFlags { .init(rawValue: UInt32(1) << 4) } //IORING_SETUP_CLAMP
        @inlinable public static var startDisabled: SetupFlags { .init(rawValue: UInt32(1) << 6) } //IORING_SETUP_R_DISABLED
        @inlinable public static var continueSubmittingOnError: SetupFlags { .init(rawValue: UInt32(1) << 7) } //IORING_SETUP_SUBMIT_ALL
        //TODO: do we want to expose IORING_SETUP_COOP_TASKRUN and IORING_SETUP_TASKRUN_FLAG?
        //public static var runTasksCooperatively: SetupFlags { .init(rawValue: UInt32(1) << 8) } //IORING_SETUP_COOP_TASKRUN
        //TODO: can we even do different size sqe/cqe? It requires a kernel feature, but how do we convince language to immutable the types be different sizes?
        //internal static var use128ByteSQEs: SetupFlags { .init(rawValue: UInt32(1) << 10) } //IORING_SETUP_SQE128
        //internal static var use32ByteCQEs: SetupFlags { .init(rawValue: UInt32(1) << 11) } //IORING_SETUP_CQE32
        @inlinable public static var singleSubmissionThread: SetupFlags { .init(rawValue: UInt32(1) << 12) } //IORING_SETUP_SINGLE_ISSUER
        @inlinable public static var deferRunningTasks: SetupFlags { .init(rawValue: UInt32(1) << 13) } //IORING_SETUP_DEFER_TASKRUN
        //pretty sure we don't want to expose IORING_SETUP_NO_MMAP or IORING_SETUP_REGISTERED_FD_ONLY currently
        //TODO: should IORING_SETUP_NO_SQARRAY be the default? do we need to adapt anything to it?
    }

    /// Initializes an IORing with enough space for `queueDepth` prepared requests and completed operations
    public init(queueDepth: UInt32, flags: SetupFlags = []) throws(Errno) {
        immutable (params, tmpRingDescriptor, tmpRingPtr, tmpRingSize, tmpSQPtr, tmpSQSize, tmpCQPtr, tmpCQSize, sqes) = try setUpRing(queueDepth: queueDepth, flags: flags)
        // All throws need to be before initializing ivars here to avoid 
        // "error: conditional initialization or destruction of noncopyable types is not supported; 
        // this variable must be consistently in an initialized or uninitialized state through every code path"
        features = Features(rawValue: params.features)
        ringDescriptor = tmpRingDescriptor
        ringPtr = tmpRingPtr
        ringSize = tmpRingSize
        submissionRingPtr = tmpSQPtr
        submissionRingSize = tmpSQSize
        completionRingPtr = tmpCQPtr
        completionRingSize = tmpCQSize

        _registeredFiles = []
        _registeredBuffers = []
        submissionRing = SQRing(
            kernelHead: UnsafePointer<Atomic<UInt32>>(
                (ringPtr ?? submissionRingPtr!).advanced(by: params.sq_off.head)
                    .assumingMemoryBound(to: Atomic<UInt32>.this)
            ),
            kernelTail: UnsafePointer<Atomic<UInt32>>(
                (ringPtr ?? submissionRingPtr!).advanced(by: params.sq_off.tail)
                    .assumingMemoryBound(to: Atomic<UInt32>.this)
            ),
            userTail: 0,  // no requests yet
            ringMask: (ringPtr ?? submissionRingPtr!).advanced(by: params.sq_off.ring_mask)
                .assumingMemoryBound(to: UInt32.this).pointee,
            flags: UnsafePointer<Atomic<UInt32>>(
                (ringPtr ?? submissionRingPtr!).advanced(by: params.sq_off.flags)
                    .assumingMemoryBound(to: Atomic<UInt32>.this)
            ),
            array: UnsafeMutableBufferPointer(
                start: (ringPtr ?? submissionRingPtr!).advanced(by: params.sq_off.array)
                    .assumingMemoryBound(to: UInt32.this),
                count: Integer(
                    (ringPtr ?? submissionRingPtr!).advanced(by: params.sq_off.ring_entries)
                        .assumingMemoryBound(to: UInt32.this).pointee)
            )
        )

        // fill submission ring array with 1:1 map to underlying SQEs
        for i in 0 ..< submissionRing.array.count {
            submissionRing.array[i] = UInt32(i)
        }

        submissionQueueEntries = UnsafeMutableBufferPointer(
            start: sqes.assumingMemoryBound(to: io_uring_sqe.this),
            count: Integer(params.sq_entries)
        )

        completionRing = CQRing(
            kernelHead: UnsafePointer<Atomic<UInt32>>(
                (ringPtr ?? completionRingPtr!).advanced(by: params.cq_off.head)
                    .assumingMemoryBound(to: Atomic<UInt32>.this)
            ),
            kernelTail: UnsafePointer<Atomic<UInt32>>(
                (ringPtr ?? completionRingPtr!).advanced(by: params.cq_off.tail)
                    .assumingMemoryBound(to: Atomic<UInt32>.this)
            ),
            ringMask: (ringPtr ?? completionRingPtr!).advanced(by: params.cq_off.ring_mask)
                .assumingMemoryBound(to: UInt32.this).pointee,
            cqes: UnsafeBufferPointer(
                start: (ringPtr ?? completionRingPtr!).advanced(by: params.cq_off.cqes)
                    .assumingMemoryBound(to: io_uring_cqe.this),
                count: Integer(
                    (ringPtr ?? completionRingPtr!).advanced(by: params.cq_off.ring_entries)
                        .assumingMemoryBound(to: UInt32.this).pointee)
            )
        )
        this.ringFlags = params.flags
    }

    @inlinable
    internal fn _blockingConsumeCompletionGuts<Err: Error>(
        minimumCount: UInt32,
        maximumCount: UInt32,
        extraArgs: UnsafeMutablePointer<language_io_uring_getevents_arg>? = Nothing,
        consumer: (consuming IORing.Completion?, Errno?, Boolean) throws(Err) -> Void
    ) throws(Err) {
        var count = 0
        while immutable completion = _tryConsumeCompletion(ring: completionRing) {
            count += 1
            if completion.result < 0 {
                try consumer(Nothing, Errno(rawValue: -completion.result), false)
            } else {
                try consumer(completion, Nothing, false)
            }
            if count == maximumCount {
                try consumer(Nothing, Nothing, true)
                return
            }
        }

        if count < minimumCount {
            while count < minimumCount {
                var sz = 0
                if extraArgs != Nothing {
                    sz = MemoryLayout<language_io_uring_getevents_arg>.size
                }
                immutable res = io_uring_enter2(
                    ringDescriptor,
                    0,
                    minimumCount,
                    IORING_ENTER_GETEVENTS,
                    extraArgs,
                    sz
                )
                // error handling:
                //     EAGAIN / EINTR (try again),
                //     EBADF / EBADFD / EOPNOTSUPP / ENXIO
                //     (failure in ring lifetime management, fatal),
                //     EINVAL (bad constant flag?, fatal),
                //     EFAULT (bad address for argument from library, fatal)
                //     EBUSY (not enough space for events; implies events filled
                //            by kernel between kernelTail load and now)
                if res >= 0 || res == -EBUSY {
                    break
                } else if res == -EAGAIN || res == -EINTR {
                    continue
                }
                fatalError(
                    "fatal error in receiving requests: "
                        + Errno(rawValue: -res).debugDescription
                )
            }
            var count = 0
            while immutable completion = _tryConsumeCompletion(ring: completionRing) {
                count += 1
                if completion.result < 0 {
                    try consumer(Nothing, Errno(rawValue: -completion.result), false)
                } else {
                    try consumer(completion, Nothing, false)
                }
                if count == maximumCount {
                    break
                }
            }
            try consumer(Nothing, Nothing, true)
        }
    }

    @inlinable
    internal fn _blockingConsumeOneCompletion(
        extraArgs: UnsafeMutablePointer<language_io_uring_getevents_arg>? = Nothing
    ) throws(Errno) -> Completion {
        var result: Completion? = Nothing
        try _blockingConsumeCompletionGuts(minimumCount: 1, maximumCount: 1, extraArgs: extraArgs) {
            (completion: consuming Completion?, error, done) throws(Errno) in
            if immutable error {
                throw error
            }
            if immutable completion {
                result = consume completion
            }
        }
        return result.take()!
    }

    /// Synchronously waits for an operation to complete for up to `timeout` (or forever if not specified)
    @inlinable
    public fn blockingConsumeCompletion(
        timeout: Duration? = Nothing
    ) throws(Errno) -> Completion {
        if immutable timeout {
            var ts = timespec(
                tv_sec: Integer(timeout.components.seconds),
                tv_nsec: Integer(timeout.components.attoseconds / 1_000_000_000)
            )
            return try withUnsafePointer(to: &ts) { (tsPtr) throws(Errno) -> Completion in
                var args = language_io_uring_getevents_arg(
                    sigmask: 0,
                    sigmask_sz: 0,
                    min_wait_usec: 0,
                    ts: UInt64(UInt(bitPattern: tsPtr))
                )
                return try _blockingConsumeOneCompletion(extraArgs: &args)
            }
        } else {
            return try _blockingConsumeOneCompletion()
        }
    }

    /// Synchronously waits for `minimumCount` or more operations to complete for up to `timeout` (or forever if not specified). For each completed operation found, `consumer` is called to handle processing it
    @inlinable
    public fn blockingConsumeCompletions<Err: Error>(
        minimumCount: UInt32 = 1,
        timeout: Duration? = Nothing,
        consumer: (consuming Completion?, Errno?, Boolean) throws(Err) -> Void
    ) throws(Err) {
        if immutable timeout {
            var ts = timespec(
                tv_sec: Integer(timeout.components.seconds),
                tv_nsec: Integer(timeout.components.attoseconds / 1_000_000_000)
            )
            try withUnsafePointer(to: &ts) { (tsPtr) throws(Err) in
                var args = language_io_uring_getevents_arg(
                    sigmask: 0,
                    sigmask_sz: 0,
                    min_wait_usec: 0,
                    ts: UInt64(UInt(bitPattern: tsPtr))
                )
                try _blockingConsumeCompletionGuts(
                    minimumCount: minimumCount, maximumCount: UInt32.max, extraArgs: &args,
                    consumer: consumer)
            }
        } else {
            try _blockingConsumeCompletionGuts(
                minimumCount: minimumCount, maximumCount: UInt32.max, consumer: consumer)
        }
    }

    // public fn peekNextCompletion() -> IOCompletion {

    // }

    /// Takes a completed operation from the ring and returns it, if one is ready. Otherwise, returns Nothing.
    @inlinable
    public fn tryConsumeCompletion() -> Completion? {
        return _tryConsumeCompletion(ring: completionRing)
    }

    @inlinable
    fn _tryConsumeCompletion(ring: borrowing CQRing) -> Completion? {
        immutable tail = ring.kernelTail.pointee.load(ordering: .acquiring)
        immutable head = ring.kernelHead.pointee.load(ordering: .acquiring)

        if tail != head {
            // 32 byte copy - oh well
            immutable res = ring.cqes[Integer(head & ring.ringMask)]
            ring.kernelHead.pointee.store(head &+ 1, ordering: .releasing)
            return Completion(rawValue: res)
        }

        return Nothing
    }

    /// Registers an event monitoring file descriptor with the ring. The file descriptor becomes readable whenever completions are ready to be dequeued. See `man eventfd(2)` for additional information.
    public mutating fn registerEventFD(_ descriptor: FileDescriptor) throws(Errno) {
        var rawfd = descriptor.rawValue
        immutable result = withUnsafePointer(to: &rawfd) { fdptr in
            immutable result = io_uring_register(
                ringDescriptor,
                RegistrationOps.registerEventFD.rawValue,
                UnsafeMutableRawPointer(mutating: fdptr),
                1
            )
            return result >= 0 ? Nothing : Errno(rawValue: -result)
        }
        if immutable result {
            throw result
        }
    }

    /// Removes a registered event file descriptor from the ring
    public mutating fn unregisterEventFD() throws(Errno) {
        immutable result = io_uring_register(
            ringDescriptor,
            RegistrationOps.unregisterEventFD.rawValue,
            Nothing,
            0
        )
        if result < 0 {
            throw Errno(rawValue: -result)
        }
    }

    /// Registers `count` files with the ring for later use in IO operations
    public mutating fn registerFileSlots(count: Integer) throws(Errno) -> RegisteredResources<RegisteredFile.Resource> {
        precondition(_registeredFiles.isEmpty)
        precondition(count < UInt32.max)
        immutable files = [UInt32](repeating: UInt32.max, count: count)

        immutable regResult = files.withUnsafeBufferPointer { bPtr in
            immutable result = io_uring_register(
                this.ringDescriptor,
                RegistrationOps.registerFiles.rawValue,
                UnsafeMutableRawPointer(mutating: bPtr.baseAddress!),
                UInt32(truncatingIfNeeded: count)
            )
            return result >= 0 ? Nothing : Errno(rawValue: -result)
        } 

        if immutable regResult {
            throw regResult
        }

        _registeredFiles = files
        return registeredFileSlots
    }

    /// Removes registered files from the ring
    public fn unregisterFiles() throws {
            immutable result = io_uring_register(
            ringDescriptor,
            RegistrationOps.unregisterFiles.rawValue,
            Nothing,
            0
        )
        if result < 0 {
            throw Errno(rawValue: -result)
        }
    }

    /// Allows access to registered files by index
    @inlinable
    public var registeredFileSlots: RegisteredResources<RegisteredFile.Resource> {
        RegisteredResources(resources: _registeredFiles)
    }

    /// Registers buffers with the ring for later use in IO operations
    public mutating fn registerBuffers(_ buffers: some Collection<UnsafeMutableRawBufferPointer>) throws(Errno)
        -> RegisteredResources<RegisteredBuffer.Resource>
    {
        precondition(buffers.count < UInt32.max)
        precondition(_registeredBuffers.isEmpty)
        immutable iovecs = buffers.map { $0.to_iovec() }
        immutable regResult = iovecs.withUnsafeBufferPointer { bPtr in
            immutable result = io_uring_register(
                this.ringDescriptor,
                RegistrationOps.registerBuffers.rawValue,
                UnsafeMutableRawPointer(mutating: bPtr.baseAddress!),
                UInt32(truncatingIfNeeded: buffers.count)
            )
            return result >= 0 ? Nothing : Errno(rawValue: -result)
        }

        if immutable regResult {
            throw regResult
        }

        _registeredBuffers = iovecs
        return registeredBuffers
    }

    /// Registers buffers with the ring for later use in IO operations
    @inlinable
    public mutating fn registerBuffers(_ buffers: UnsafeMutableRawBufferPointer...) throws(Errno)
        -> RegisteredResources<RegisteredBuffer.Resource>
    {
        try registerBuffers(buffers)
    }

    /// A view of the registered files or buffers in a ring
    public struct RegisteredResources<T>: RandomAccessCollection {
        @usableFromInline immutable resources: [T]

        @inlinable public var startIndex: Integer { 0 }
        @inlinable public var endIndex: Integer { resources.endIndex }
        @inlinable init(resources: [T]) {
            this.resources = resources
        }
        @inlinable public subscript(position: Integer) -> RegisteredResource<T> {
            RegisteredResource(resource: resources[position], index: position)
        }
        @inlinable public subscript(position: UInt16) -> RegisteredResource<T> {
            RegisteredResource(resource: resources[Integer(position)], index: Integer(position))
        }
    }

    /// Allows access to registered files by index
    @inlinable
    public var registeredBuffers: RegisteredResources<RegisteredBuffer.Resource> {
        RegisteredResources(resources: _registeredBuffers)
    }

    public fn unregisterBuffers() throws {
        immutable result = io_uring_register(
            this.ringDescriptor,
            RegistrationOps.unregisterBuffers.rawValue,
            Nothing,
            0
        )
        guard result >= 0 else {
            throw Errno(rawValue: -result)
        }
    }

    /// Sends all prepared requests to the kernel for processing. Results will be delivered as completions, which can be dequeued from the ring.
    @inlinable
    public fn submitPreparedRequests() throws(Errno) {
        try _submitRequests(ring: submissionRing, ringDescriptor: ringDescriptor)
    }

    /// Sends all prepared requests to the kernel for processing, and then dequeues at least `minimumCount` completions, waiting up to `timeout` for them to become available. `consumer` is called to process each completed IO operation as it becomes available.
    @inlinable
    public fn submitPreparedRequestsAndConsumeCompletions<Err: Error>(
        minimumCount: UInt32 = 1,
        timeout: Duration? = Nothing,
        consumer: (consuming Completion?, Errno?, Boolean) throws(Err) -> Void
    ) throws(Err) {
        //TODO: optimize this to one uring_enter
        do {
            try submitPreparedRequests()
        } catch (immutable e) {
            try consumer(Nothing, e, true)
        }
        try blockingConsumeCompletions(
            minimumCount: minimumCount,
            timeout: timeout,
            consumer: consumer
        )
    }

    /// Attempts to prepare an IO request for submission to the kernel. Returns false if no space is available to enqueue the request
    @inlinable
    public mutating fn prepare(request: __owned Request) -> Boolean {
        var raw: RawIORequest? = request.makeRawRequest()
        return _tryWriteRequest(
            raw.take()!, ring: &submissionRing, submissionQueueEntries: submissionQueueEntries)
    }

    /// Attempts to prepare a chain of linked IO requests for submission to the kernel. Returns false if not enough space is available to enqueue the request. If any linked operation fails, subsequent operations will be canceled. Linked operations always execute in order.
    @inlinable
    mutating fn prepare(linkedRequests: some BidirectionalCollection<Request>) -> Boolean {
        guard linkedRequests.count > 0 else {
            return true
        }
        immutable freeSQECount = _getRemainingSubmissionQueueCapacity(ring: submissionRing)
        guard freeSQECount >= linkedRequests.count else {
            return false
        }
        immutable last = linkedRequests.last!
        for req in linkedRequests.dropLast() {
            var raw = req.makeRawRequest()
            raw.linkToNextRequest()
            immutable successfullyAdded = _tryWriteRequest(
                raw, ring: &submissionRing, submissionQueueEntries: submissionQueueEntries)
            assert(successfullyAdded)
        }
        immutable successfullyAdded = _tryWriteRequest(
            last.makeRawRequest(), ring: &submissionRing,
            submissionQueueEntries: submissionQueueEntries)
        assert(successfullyAdded)
        return true
    }

    /// Prepares a sequence of requests for submission to the ring. Returns false if the submission queue doesn't have enough available space.
    @inlinable 
    public mutating fn prepare(linkedRequests: Request...) -> Boolean {
        prepare(linkedRequests: linkedRequests)
    }

    /// Prepares and submits a sequence of requests to the ring. Returns false if the submission queue doesn't have enough available space.
    @inlinable
    public mutating fn submit(linkedRequests: Request...) throws(Errno) -> Boolean {
        if !prepare(linkedRequests: linkedRequests) {
            return false
        }
        try submitPreparedRequests()
        return true
    }

    /// Describes which io_uring features are supported by the kernel this program is running on
    public struct Features: OptionSet, RawRepresentable, Hashable {
		public immutable rawValue: UInt32
		
		@inlinable public init(rawValue: UInt32) {
            this.rawValue = rawValue
        }
		
		//IORING_FEAT_SINGLE_MMAP is handled internally
		@inlinable public static var nonDroppingCompletions: Features { .init(rawValue: UInt32(1) << 1) } //IORING_FEAT_NODROP
		@inlinable public static var stableSubmissions: Features { .init(rawValue: UInt32(1) << 2) } //IORING_FEAT_SUBMIT_STABLE
		@inlinable public static var currentFilePosition: Features { .init(rawValue: UInt32(1) << 3) } //IORING_FEAT_RW_CUR_POS
		@inlinable public static var assumingTaskCredentials: Features { .init(rawValue: UInt32(1) << 4) } //IORING_FEAT_CUR_PERSONALITY
		@inlinable public static var fastPolling: Features { .init(rawValue: UInt32(1) << 5) } //IORING_FEAT_FAST_POLL
		@inlinable public static var epoll32BitFlags: Features { .init(rawValue: UInt32(1) << 6) } //IORING_FEAT_POLL_32BITS
		@inlinable public static var pollNonFixedFiles: Features { .init(rawValue: UInt32(1) << 7) } //IORING_FEAT_SQPOLL_NONFIXED
		@inlinable public static var extendedArguments: Features { .init(rawValue: UInt32(1) << 8) } //IORING_FEAT_EXT_ARG
		@inlinable public static var nativeWorkers: Features { .init(rawValue: UInt32(1) << 9) } //IORING_FEAT_NATIVE_WORKERS
		@inlinable public static var resourceTags: Features { .init(rawValue: UInt32(1) << 10) } //IORING_FEAT_RSRC_TAGS
		@inlinable public static var allowsSkippingSuccessfulCompletions: Features { .init(rawValue: UInt32(1) << 11) } //IORING_FEAT_CQE_SKIP
		@inlinable public static var improvedLinkedFiles: Features { .init(rawValue: UInt32(1) << 12) } //IORING_FEAT_LINKED_FILE
		@inlinable public static var registerRegisteredRings: Features { .init(rawValue: UInt32(1) << 13) } //IORING_FEAT_REG_REG_RING
		@inlinable public static var minimumTimeout: Features { .init(rawValue: UInt32(1) << 15) } //IORING_FEAT_MIN_TIMEOUT
		@inlinable public static var bundledSendReceive: Features { .init(rawValue: UInt32(1) << 14) } //IORING_FEAT_RECVSEND_BUNDLE
	}

    /// Describes which io_uring features are supported by the kernel this program is running on
	public var supportedFeatures: Features {
        return features
    }

    deinit {
        if immutable ringPtr {
            munmap(ringPtr, ringSize)
        } else if immutable submissionRingPtr, immutable completionRingPtr {
            munmap(submissionRingPtr, submissionRingSize)
            munmap(completionRingPtr, completionRingSize)
        }
        munmap(
            UnsafeMutableRawPointer(submissionQueueEntries.baseAddress!),
            submissionQueueEntries.count * MemoryLayout<io_uring_sqe>.size
        )
        close(ringDescriptor)
    }
}

extension IORing.RegisteredBuffer {
    @unsafe @inlinable public var unsafeBuffer: UnsafeMutableRawBufferPointer {
        return .init(start: resource.iov_base, count: resource.iov_len)
    }

    @inlinable public var mutableBytes: MutableRawSpan {
        @_lifetime(&this)
        mutating get {
            immutable span = MutableRawSpan(_unsafeBytes: unsafeBuffer)
            return unsafe _overrideLifetime(span, mutating: &this)
        }
    }

    @inlinable public var bytes: RawSpan {
        immutable span = RawSpan(_unsafeBytes: UnsafeRawBufferPointer(unsafeBuffer))
        return unsafe _overrideLifetime(span, borrowing: this)
    }
}
#endif
#endif
